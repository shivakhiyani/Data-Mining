# -*- coding: utf-8 -*-
"""20ngQ7.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/145UOh-UxxN4g8JEdPUlUSma7IM1n_4iW
"""

from sklearn.preprocessing import normalize
from sklearn.metrics import silhouette_score
from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances

import numpy as np

def finding_neighbors(data,item_index,eps):
  neighbor_index = set()
  for index in range(data.shape[0]):
    if np.linalg.norm(data[item_index]-data[index])<eps:
      neighbor_index.add(index)
  return neighbor_index

def main_dbscan(data, eps, min_pts):
  label = [0]*data.shape[0]
  cluster_no = 0
  visited_index = []
  noise_temp = []
  for index in range(data.shape[0]):
    if index not in visited_index:
      visited_index.append(index)
      neighbor_index = finding_neighbors(data, index, eps)
      if len(neighbor_index)<min_pts:
        noise_temp.append(index)
      else:
        cluster_no +=1
        label[index] = cluster_no
        for item_index in neighbor_index.copy():
          if item_index in noise_temp:
            label[item_index] = cluster_no
          elif item_index not in visited_index:
            visited_index.append(item_index)
            neighbor_index_add = finding_neighbors(data, index, eps)
            label[item_index]= cluster_no
            if len(neighbor_index_add)>=min_pts:
              neighbor_index.update(neighbor_index_add)
  return label, cluster_no +1

def clustering_performance(labels, clusters):
  count_per_cluster = []
  n=0
  bins = np.concatenate((np.unique(labels),[np.max(np.unique(labels))+1]),axis=0)
  for cluster in np.unique(clusters):
    #  (unique,counts)=  np.unique(labels[clusters==cluster],return_counts=True)
    #  count_per_cluster.append(counts)
     count_per_cluster.append(np.histogram(labels[clusters==cluster],bins=bins)[0])
  total_true = 0
 
  for cluster_count in count_per_cluster:
    total_true += np.max(cluster_count)
    purity = total_true / len(clusters)
  
  gini_num = 0
  gini_column=[]
  gini_den = 0
  for cluster_count in count_per_cluster:
    total  = np.sum(cluster_count)
    gini_den+=total
    sum = 0
    for count in cluster_count:
      sum +=pow(count/total,2)
    gini_column.append(1-sum)
    gini_num +=((1-sum)*total)
  gini_coefficient = gini_num/gini_den
  return purity, gini_coefficient
 
  #return gini_column
# print(clustering_performance(train_label,clusters))

from sklearn.datasets import fetch_20newsgroups
newsgroups_train = fetch_20newsgroups(subset='train')
print(newsgroups_train.target[:10])
newsgroups_test = fetch_20newsgroups(subset='test')
#newsgroups = fetch_20newsgroups()

from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer(stop_words='english',min_df = 2)
vector1 = vectorizer.fit_transform(newsgroups_train.data)
vector_1=vector1

vector2 = vectorizer.transform(newsgroups_train.data)
nglabel = newsgroups_train.target[:1500]
data=vector2.toarray() 
ngdata = data[:1500]

label,cluster_no = main_dbscan(ngdata ,eps=1.4,min_pts=150)
print(cluster_no)

from collections import Counter
print(Counter(label))
print(label)

ec_score=euclidean_distances(ngdata)
print(np.mean(ec_score))

clustering_performance(nglabel, label)

label,cluster_no = main_dbscan(ngdata ,eps=1.4,min_pts=100)
print(cluster_no)

clustering_performance(nglabel, label)

label,cluster_no = main_dbscan(ngdata ,eps=1.35,min_pts=100)
print(cluster_no)

clustering_performance(nglabel, label)