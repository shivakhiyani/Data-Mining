# -*- coding: utf-8 -*-
"""fashion mnistQ7.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/142Xa2I2wrxN7zQOuGQ626AgwF7_Y0gwG
"""

from sklearn.preprocessing import normalize
from sklearn.metrics import silhouette_score
from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances

import numpy as np

def finding_neighbors(data,item_index,eps):
  neighbor_index = set()
  for index in range(data.shape[0]):
    if np.linalg.norm(data[item_index]-data[index])<eps:
      neighbor_index.add(index)
  return neighbor_index

def main_dbscan(data, eps, min_pts):
  label = [0]*data.shape[0]
  cluster_no = 0
  visited_index = []
  noise_temp = []
  for index in range(data.shape[0]):
    if index not in visited_index:
      visited_index.append(index)
      neighbor_index = finding_neighbors(data, index, eps)
      if len(neighbor_index)<min_pts:
        noise_temp.append(index)
      else:
        cluster_no +=1
        label[index] = cluster_no
        for item_index in neighbor_index.copy():
          if item_index in noise_temp:
            label[item_index] = cluster_no
          elif item_index not in visited_index:
            visited_index.append(item_index)
            neighbor_index_add = finding_neighbors(data, index, eps)
            label[item_index]= cluster_no
            if len(neighbor_index_add)>=min_pts:
              neighbor_index.update(neighbor_index_add)
  return label, cluster_no +1

def clustering_performance(labels, clusters):
  count_per_cluster = []
  n=0
  bins = np.concatenate((np.unique(labels),[np.max(np.unique(labels))+1]),axis=0)
  for cluster in np.unique(clusters):
    #  (unique,counts)=  np.unique(labels[clusters==cluster],return_counts=True)
    #  count_per_cluster.append(counts)
     count_per_cluster.append(np.histogram(labels[clusters==cluster],bins=bins)[0])
  total_true = 0
 
  for cluster_count in count_per_cluster:
    total_true += np.max(cluster_count)
    purity = total_true / len(clusters)
  
  gini_num = 0
  gini_column=[]
  gini_den = 0
  for cluster_count in count_per_cluster:
    total  = np.sum(cluster_count)
    gini_den+=total
    sum = 0
    for count in cluster_count:
      sum +=pow(count/total,2)
    gini_column.append(1-sum)
    gini_num +=((1-sum)*total)
  gini_coefficient = gini_num/gini_den
  return purity, gini_coefficient
  #return gini_column
# print(clustering_performance(train_label,clusters))

import numpy as np
from tensorflow import  keras
#from keras.datasets import fashion_mnist
# load dataset
(train_images, train_labels), (test_images, test_labels) = keras.datasets.fashion_mnist.load_data()
# summarize dataset shape
print('Train', train_images.shape, train_labels.shape)
print('Test', (test_images.shape, test_labels.shape))
# summarize pixel values
print('Train', train_images.min(), train_images.max(), train_images.mean(), train_images.std())
print('Test', test_images.min(), test_images.max(), test_images.mean(), test_images.std())
train_vector = np.reshape(train_images, (60000, 28 * 28))
test_vector = np.reshape(test_images, (10000, 28 * 28))
test_data = train_vector[:10000]
test_label=train_labels[:10000]

ec_score=euclidean_distances(test_data)
print(np.mean(ec_score))

label,cluster_no = main_dbscan(test_data ,eps=2950,min_pts=50)
print(cluster_no)



from collections import Counter
print(Counter(label))

clustering_performance(test_label, label)

label,cluster_no = main_dbscan(test_data ,eps=2950,min_pts=100)
print(cluster_no)

clustering_performance(test_label, label)